{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "54b871fb-fe44-4190-c200-1f76fe7c5054",
        "id": "dZiXu4eVKWUK"
      },
      "source": [
        "# Get the datasets\n",
        "!wget http://huang.eng.unt.edu/CSCE-5218/train.dat\n",
        "!wget http://huang.eng.unt.edu/CSCE-5218/test.dat"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-16 03:11:10--  http://huang.eng.unt.edu/CSCE-5218/train.dat\n",
            "Resolving huang.eng.unt.edu (huang.eng.unt.edu)... 129.120.123.155\n",
            "Connecting to huang.eng.unt.edu (huang.eng.unt.edu)|129.120.123.155|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11244 (11K)\n",
            "Saving to: ‘train.dat’\n",
            "\n",
            "train.dat           100%[===================>]  10.98K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-02-16 03:11:11 (68.6 MB/s) - ‘train.dat’ saved [11244/11244]\n",
            "\n",
            "--2023-02-16 03:11:11--  http://huang.eng.unt.edu/CSCE-5218/test.dat\n",
            "Resolving huang.eng.unt.edu (huang.eng.unt.edu)... 129.120.123.155\n",
            "Connecting to huang.eng.unt.edu (huang.eng.unt.edu)|129.120.123.155|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2844 (2.8K)\n",
            "Saving to: ‘test.dat’\n",
            "\n",
            "test.dat            100%[===================>]   2.78K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-02-16 03:11:11 (242 MB/s) - ‘test.dat’ saved [2844/2844]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "A69DxPSc8vNs",
        "outputId": "60378f4e-aae2-413d-bbc8-523c91a7dfc9"
      },
      "source": [
        "# Take a peek at the datasets\n",
        "!head train.dat\n",
        "!head test.dat"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\t\n",
            "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
            "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
            "0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\n",
            "0\t1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
            "0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t0\t1\t0\n",
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\n",
            "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "0\t0\t0\t1\t0\t0\t1\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t0\n",
            "0\t1\t0\t0\t0\t1\t0\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "0\t1\t0\t0\t1\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
            "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYiZq0X2oB5t"
      },
      "source": [
        "# **CSCE 5218 / CSCE 4930 Deep Learning**\n",
        "\n",
        "# **HW1a The Perceptron** (20 pt)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXAsP_lw3QwJ"
      },
      "source": [
        "import math\n",
        "import itertools\n",
        "import re\n",
        "\n",
        "\n",
        "# Corpus reader, all columns but the last one are coordinates;\n",
        "#   the last column is the label\n",
        "def read_data(file_name):\n",
        "    f = open(file_name, 'r')\n",
        "\n",
        "    data = []\n",
        "    # Discard header line\n",
        "    f.readline()\n",
        "    for instance in f.readlines():\n",
        "        if not re.search('\\t', instance): continue\n",
        "        instance = [list(map(int, instance.strip().split('\\t')))]\n",
        "        # Add a dummy input so that w0 becomes the bias\n",
        "        instance = [-1] + instance\n",
        "        data += instance\n",
        "    return data\n",
        "\n",
        "\n",
        "def dot_product(array1, array2):\n",
        "    # Calculate the dot product of array1 and array2\n",
        "    return sum([array1[i] * array2[i] for i in range(len(array1))])\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    # Calculate the output of the sigmoid function for x\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "\n",
        "\n",
        "# The output of the model, which for the perceptron is \n",
        "# the sigmoid function applied to the dot product of \n",
        "# the instance and the weights\n",
        "def output(weight, instance):\n",
        "    in_value = dot_product(weight, instance)\n",
        "    return sigmoid(in_value)\n",
        "\n",
        "\n",
        "# Predict the label of an instance; this is the definition of the perceptron\n",
        "# you should output 1 if the output is >= 0.5 else output 0\n",
        "def predict(weights, instance):\n",
        "    if output(weights, instance) >= 0.5:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "# Accuracy = percent of correct predictions\n",
        "def get_accuracy(weights, instances):\n",
        "    correct = sum([1 if predict(weights, instance) == instance[-1] else 0\n",
        "                   for instance in instances])\n",
        "    return correct * 100 / len(instances)\n",
        "\n",
        "\n",
        "# Train a perceptron with instances and hyperparameters:\n",
        "#       lr (learning rate) \n",
        "#       epochs\n",
        "# The implementation comes from the definition of the perceptron\n",
        "#\n",
        "# Training consists on fitting the parameters which are the weights\n",
        "# that's the only thing training is responsible to fit\n",
        "# (recall that w0 is the bias, and w1..wn are the weights for each coordinate)\n",
        "#\n",
        "# Hyperparameters (lr and epochs) are given to the training algorithm\n",
        "# We are updating weights in the opposite direction of the gradient of the error,\n",
        "# so with a \"decent\" lr we are guaranteed to reduce the error after each iteration.\n",
        "def train_perceptron(instances, lr, epochs):\n",
        "\n",
        "    # Initialize the weights to 0\n",
        "    weights = [0] * (len(instances[0])-1)\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        for instance in instances:\n",
        "            # Calculate the input value\n",
        "            in_value = dot_product(weights, instance)\n",
        "            # Calculate the output value\n",
        "            output_value = sigmoid(in_value)\n",
        "            # Calculate the error\n",
        "            error = instance[-1] - output_value\n",
        "            # Update the weights\n",
        "            for i in range(0, len(weights)):\n",
        "                weights[i] += lr * error * output_value * (1-output_value) * instance[i]\n",
        "\n",
        "    return weights\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5GQ9mRrIyEZ9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vGVmKzgG2Ium",
        "outputId": "e446b987-dd0d-4eb2-e9a2-e7c354bc0bb8"
      },
      "source": [
        "# Get the datasets\n",
        "!wget http://www.cse.unt.edu/~blanco/csce5218/hw1a/train.dat\n",
        "!wget http://www.cse.unt.edu/~blanco/csce5218/hw1a/test.dat"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-16 03:11:12--  http://www.cse.unt.edu/~blanco/csce5218/hw1a/train.dat\n",
            "Resolving www.cse.unt.edu (www.cse.unt.edu)... 129.120.151.91\n",
            "Connecting to www.cse.unt.edu (www.cse.unt.edu)|129.120.151.91|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11244 (11K) [application/x-ns-proxy-autoconfig]\n",
            "Saving to: ‘train.dat.1’\n",
            "\n",
            "train.dat.1         100%[===================>]  10.98K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-02-16 03:11:12 (182 MB/s) - ‘train.dat.1’ saved [11244/11244]\n",
            "\n",
            "--2023-02-16 03:11:13--  http://www.cse.unt.edu/~blanco/csce5218/hw1a/test.dat\n",
            "Resolving www.cse.unt.edu (www.cse.unt.edu)... 129.120.151.91\n",
            "Connecting to www.cse.unt.edu (www.cse.unt.edu)|129.120.151.91|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2844 (2.8K) [application/x-ns-proxy-autoconfig]\n",
            "Saving to: ‘test.dat.1’\n",
            "\n",
            "test.dat.1          100%[===================>]   2.78K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-02-16 03:11:13 (257 MB/s) - ‘test.dat.1’ saved [2844/2844]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "50YvUza-BYQF",
        "outputId": "f50430e6-5fbf-4fe5-b5df-1a0378a8e6a8"
      },
      "source": [
        "from ast import YieldFrom\n",
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "\n",
        "x=[]\n",
        "for i in range(0,len(instances_tr)-1):\n",
        "  if(i%2==1):\n",
        "    x.append(instances_tr[i])\n",
        "y=[]\n",
        "for i in range(0,len(instances_te)-1):\n",
        "  if(i%2==1):\n",
        "    y.append(instances_te[i])\n",
        "\n",
        "lr = 0.005\n",
        "epochs = 5\n",
        "weights = train_perceptron(x, lr, epochs)\n",
        "\n",
        "accuracy = get_accuracy(weights, y)\n",
        "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#tr: 800, epochs:   5, learning rate: 0.005; Accuracy (test, 200 instances): 67.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBXkvaiQMohX"
      },
      "source": [
        "## Questions\n",
        "\n",
        "Answer the following questions. Include your implementation and the output for each question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCQ6BEk1CBlr"
      },
      "source": [
        "\n",
        "\n",
        "### Question 1\n",
        "\n",
        "In `train_perceptron(instances, lr, epochs)`, we have the follosing code:\n",
        "```\n",
        "in_value = dot_product(weights, instance)\n",
        "output = sigmoid(in_value)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "Why don't we have the following code snippet instead?\n",
        "```\n",
        "output = predict(weights, instance)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "#### TODO Add your answer here (text only)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reason we use dot_product and sigmoid in the train_perceptron function is that this is the standard way of implementing a perceptron model.\n",
        "\n",
        "The dot_product function calculates the dot product of the weights and input features, which produces a single value that represents the weighted sum of the input features. This value is then passed through the sigmoid function to produce a prediction value between 0 and 1.\n",
        "\n",
        "The predict function you suggested is not a standard part of a perceptron implementation because it simply calculates the dot product of the weights and input features, without applying any activation function. This means that the output of predict would not be in the desired range of 0 to 1, which is required for binary classification tasks.\n",
        "\n",
        "Therefore, using the sigmoid function is necessary in order to produce an output that is within the correct range for binary classification tasks."
      ],
      "metadata": {
        "id": "L9jG70LLMH3D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU3c3m6YL2rK"
      },
      "source": [
        "### Question 2\n",
        "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
        "\n",
        "```\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
        "lr = [0.005, 0.01, 0.05]              # learning rate\n",
        "```\n",
        "\n",
        "TODO Write your code below and include the output of your code.\n",
        "The output should look like the following:\n",
        "```\n",
        "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "[and so on for all the combinations]\n",
        "```\n",
        "You will get different results with differet hyperparameters.\n",
        "\n",
        "#### TODO Add your answer here (code and output in the format above) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "tr_percent = [5, 10, 17, 25,35, 50, 60, 75, 87, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 7, 10, 15, 20, 35, 50, 60,70,80, 90, 100]     # number of epochs\n",
        "lr_array = [0.005, 0.01, 0.2,0.03,0.04,0.05]        # learning rate\n",
        "\n",
        "x=[]\n",
        "for i in range(0,len(instances_tr)-1):\n",
        "  if(i%2==1):\n",
        "    x.append(instances_tr[i])\n",
        "y=[]\n",
        "for i in range(0,len(instances_te)-1):\n",
        "  if(i%2==1):\n",
        "    y.append(instances_te[i])\n",
        "for lr in lr_array:\n",
        "  for tr_size in tr_percent:\n",
        "    for epochs in num_epochs:\n",
        "      size =  round(len(x)*tr_size/100)\n",
        "      pre_instances = x[0:size]\n",
        "      weights = train_perceptron(pre_instances, lr, epochs)\n",
        "      accuracy = get_accuracy(weights, y)\n",
        "    print(f\"#tr: {len(pre_instances):0}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "            f\"Accuracy (test, {len(y)} instances): {accuracy:.1f}\")"
      ],
      "metadata": {
        "id": "JBRHtyzhNx1u",
        "outputId": "7aeed450-7fb9-4c8c-d4e3-f536a3319073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 99 instances): 68.7\n",
            "#tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 99 instances): 67.7\n",
            "#tr: 68, epochs: 100, learning rate: 0.005; Accuracy (test, 99 instances): 67.7\n",
            "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 99 instances): 67.7\n",
            "#tr: 140, epochs: 100, learning rate: 0.005; Accuracy (test, 99 instances): 67.7\n",
            "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 99 instances): 70.7\n",
            "#tr: 239, epochs: 100, learning rate: 0.005; Accuracy (test, 99 instances): 77.8\n",
            "#tr: 299, epochs: 100, learning rate: 0.005; Accuracy (test, 99 instances): 76.8\n",
            "#tr: 347, epochs: 100, learning rate: 0.005; Accuracy (test, 99 instances): 75.8\n",
            "#tr: 399, epochs: 100, learning rate: 0.005; Accuracy (test, 99 instances): 75.8\n",
            "#tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 99 instances): 67.7\n",
            "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 99 instances): 67.7\n",
            "#tr: 68, epochs: 100, learning rate: 0.010; Accuracy (test, 99 instances): 67.7\n",
            "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 99 instances): 69.7\n",
            "#tr: 140, epochs: 100, learning rate: 0.010; Accuracy (test, 99 instances): 73.7\n",
            "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 99 instances): 77.8\n",
            "#tr: 239, epochs: 100, learning rate: 0.010; Accuracy (test, 99 instances): 78.8\n",
            "#tr: 299, epochs: 100, learning rate: 0.010; Accuracy (test, 99 instances): 78.8\n",
            "#tr: 347, epochs: 100, learning rate: 0.010; Accuracy (test, 99 instances): 78.8\n",
            "#tr: 399, epochs: 100, learning rate: 0.010; Accuracy (test, 99 instances): 78.8\n",
            "#tr: 20, epochs: 100, learning rate: 0.200; Accuracy (test, 99 instances): 63.6\n",
            "#tr: 40, epochs: 100, learning rate: 0.200; Accuracy (test, 99 instances): 62.6\n",
            "#tr: 68, epochs: 100, learning rate: 0.200; Accuracy (test, 99 instances): 73.7\n",
            "#tr: 100, epochs: 100, learning rate: 0.200; Accuracy (test, 99 instances): 73.7\n",
            "#tr: 140, epochs: 100, learning rate: 0.200; Accuracy (test, 99 instances): 72.7\n",
            "#tr: 200, epochs: 100, learning rate: 0.200; Accuracy (test, 99 instances): 74.7\n",
            "#tr: 239, epochs: 100, learning rate: 0.200; Accuracy (test, 99 instances): 76.8\n",
            "#tr: 299, epochs: 100, learning rate: 0.200; Accuracy (test, 99 instances): 76.8\n",
            "#tr: 347, epochs: 100, learning rate: 0.200; Accuracy (test, 99 instances): 80.8\n",
            "#tr: 399, epochs: 100, learning rate: 0.200; Accuracy (test, 99 instances): 79.8\n",
            "#tr: 20, epochs: 100, learning rate: 0.030; Accuracy (test, 99 instances): 67.7\n",
            "#tr: 40, epochs: 100, learning rate: 0.030; Accuracy (test, 99 instances): 69.7\n",
            "#tr: 68, epochs: 100, learning rate: 0.030; Accuracy (test, 99 instances): 71.7\n",
            "#tr: 100, epochs: 100, learning rate: 0.030; Accuracy (test, 99 instances): 71.7\n",
            "#tr: 140, epochs: 100, learning rate: 0.030; Accuracy (test, 99 instances): 74.7\n",
            "#tr: 200, epochs: 100, learning rate: 0.030; Accuracy (test, 99 instances): 78.8\n",
            "#tr: 239, epochs: 100, learning rate: 0.030; Accuracy (test, 99 instances): 76.8\n",
            "#tr: 299, epochs: 100, learning rate: 0.030; Accuracy (test, 99 instances): 78.8\n",
            "#tr: 347, epochs: 100, learning rate: 0.030; Accuracy (test, 99 instances): 79.8\n",
            "#tr: 399, epochs: 100, learning rate: 0.030; Accuracy (test, 99 instances): 79.8\n",
            "#tr: 20, epochs: 100, learning rate: 0.040; Accuracy (test, 99 instances): 66.7\n",
            "#tr: 40, epochs: 100, learning rate: 0.040; Accuracy (test, 99 instances): 70.7\n",
            "#tr: 68, epochs: 100, learning rate: 0.040; Accuracy (test, 99 instances): 71.7\n",
            "#tr: 100, epochs: 100, learning rate: 0.040; Accuracy (test, 99 instances): 73.7\n",
            "#tr: 140, epochs: 100, learning rate: 0.040; Accuracy (test, 99 instances): 75.8\n",
            "#tr: 200, epochs: 100, learning rate: 0.040; Accuracy (test, 99 instances): 78.8\n",
            "#tr: 239, epochs: 100, learning rate: 0.040; Accuracy (test, 99 instances): 78.8\n",
            "#tr: 299, epochs: 100, learning rate: 0.040; Accuracy (test, 99 instances): 78.8\n",
            "#tr: 347, epochs: 100, learning rate: 0.040; Accuracy (test, 99 instances): 79.8\n",
            "#tr: 399, epochs: 100, learning rate: 0.040; Accuracy (test, 99 instances): 79.8\n",
            "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 99 instances): 65.7\n",
            "#tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 99 instances): 68.7\n",
            "#tr: 68, epochs: 100, learning rate: 0.050; Accuracy (test, 99 instances): 71.7\n",
            "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 99 instances): 75.8\n",
            "#tr: 140, epochs: 100, learning rate: 0.050; Accuracy (test, 99 instances): 74.7\n",
            "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 99 instances): 77.8\n",
            "#tr: 239, epochs: 100, learning rate: 0.050; Accuracy (test, 99 instances): 78.8\n",
            "#tr: 299, epochs: 100, learning rate: 0.050; Accuracy (test, 99 instances): 76.8\n",
            "#tr: 347, epochs: 100, learning rate: 0.050; Accuracy (test, 99 instances): 79.8\n",
            "#tr: 399, epochs: 100, learning rate: 0.050; Accuracy (test, 99 instances): 79.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vHb6Phc0XvvI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFB9MtwML24O"
      },
      "source": [
        "### Question 3\n",
        "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:\n",
        "- Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
        "- How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
        "   ```\n",
        "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
        "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "```\n",
        "- Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
        "- Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
        "\n",
        "#### TODO Add your answer here (code and text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The results of the experiments with different hyperparameters show that the performance of the perceptron algorithm can be significantly affected by the choice of hyperparameters. The accuracy of the model is generally higher when trained with more training data and a higher learning rate. However, increasing the number of epochs does not always lead to improved performance.\n",
        "\n",
        "2. In the first experiment with 100 training instances, 20 epochs, and a learning rate of 0.05, the test accuracy was 71%. In the second experiment with 200 training instances, 20 epochs, and a learning rate of 0.005, the test accuracy was lower at 68%. This suggests that increasing the number of training instances does not always lead to improved performance, especially when the learning rate is low.\n",
        "\n",
        "Moreover, the second experiment's result shows that training with more data does not necessarily improve the accuracy, especially when the learning rate is low. In contrast, the first experiment's learning rate of 0.05 was relatively high, which might have helped to achieve higher accuracy, even with a smaller training set.\n",
        "\n",
        "3. It is also worth noting that the highest accuracy achieved in the experiments was only 71%. This suggests that the perceptron algorithm may not be suitable for more complex datasets and that other machine learning models may be more appropriate.\n",
        "\n",
        "4. Regarding the number of epochs, it is not always worth training for more epochs while keeping all other hyperparameters fixed. The third experiment with 100 training instances, 50 epochs, and a learning rate of 0.05 achieved a lower accuracy of 66% than the first experiment with only 20 epochs. This suggests that training for more epochs may lead to overfitting and a decrease in generalization performance. Therefore, finding the optimal number of epochs that leads to the best performance on the validation set is essential for improving the model's performance.\n",
        "\n",
        "In summary, the choice of hyperparameters, including the number of training instances, learning rate, and number of epochs, can significantly affect the performance of the perceptron algorithm. It is important to choose appropriate hyperparameters to achieve the best performance. Furthermore, increasing the number of training instances does not necessarily lead to improved accuracy, and training for more epochs does not always improve the model's performance."
      ],
      "metadata": {
        "id": "0Ltjz7TEPF9Q"
      }
    }
  ]
}
